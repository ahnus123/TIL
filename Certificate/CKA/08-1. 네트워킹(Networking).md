# 8-1. 네트워킹(Networking)

## Explore Environment

### Cluster Networking

+ 마스터노드 <-> 워크노드에는 network에 연결된 network interface가 1개 이상 있음

+ 각 interface에는 주소가 설정되어야 함

+ host에는 각각의 host 이름 + MAC 주소가 설정되어야 함

  | 마스터노드                                       | 6443 for the API server |
  | ------------------------------------------------ | ----------------------- |
  | kubelets on the master and worker node           | 10250                   |
  | kube-scheduler                                   | 10251                   |
  | kube-controller-manager                          | 10252                   |
  | (외부 접근을 위해) 워커노드를 서비스에 노출할 때 | 30000 ~ 32767           |
  | ETCD 서버                                        | 2379                    |

+ 관련 명령어

  + ip 주소 + 특성에 관한 정보 조회: `ip addr` / `ip a`

    <img src="https://user-images.githubusercontent.com/33214969/161440566-1150c076-ae46-4b2d-8b70-4810b478c587.png" alt="Network - 명령어(ip addr).png" width="50%;" />

  + 모든 네트워크 Interface/Bridge 조회 : `ip link`

    <img src="https://user-images.githubusercontent.com/33214969/161440570-41e7d156-f1b0-48eb-bd6a-5ab0187f1929.png" alt="Network - 명령어(ip link).png" width="50%;" />

    + 마스터 노드 상의의 인터페이스의 MAC 주소 조회 : `ip link show [ip]`

      <img src="https://user-images.githubusercontent.com/33214969/161440569-379b3606-ae5a-40b9-bb34-ce31d7e5593c.png" alt="Network - 명령어(ip link show).png" width="50%;" />

  + route 테이블 조회 : `ip route` /  `ip route show [Gateway명]`

    <img src="https://user-images.githubusercontent.com/33214969/161440572-a8fc9276-0dde-4962-8097-83a17ee3c4c0.png" alt="Network - 명령어(ip route).png" width="50%;" />

    <img src="https://user-images.githubusercontent.com/33214969/161440571-328806ea-c4dc-47fe-8f9c-236f309eeabb.png" alt="Network - 명령어(ip route show).png" width="30%;" />

  + ip forward 조회 : `cat /proc/sys/net/ipv4/ip_forward`

    <img src="https://user-images.githubusercontent.com/33214969/161440568-87edba65-e80b-4712-ac32-f22e5d4d9907.png" alt="Network - 명령어(ip forward).png" width="40%;" />

  + 노드의 IP/MAC 주소 확인 : `arp [node명]`

    <img src="https://user-images.githubusercontent.com/33214969/161440563-382a832e-8f01-4a8e-a475-7c28ed24184e.png" alt="Network - 명령어(arp).png" width="50%;" />

  + ip/port 확인 : `netstat -nplt`

    <img src="https://user-images.githubusercontent.com/33214969/161440574-474bf0f7-01e9-41c5-a35d-e1ae167a4374.png" alt="Network - 명령어(netstat -nplt).png" width="50%;" />

  + 접속 확인 : `netstat -anp | grep [확인할 프로그램명]`

    <img src="https://user-images.githubusercontent.com/33214969/161440573-33cdbef4-2dd5-4ee7-8706-11709bd9e14c.png" alt="Network - 명령어(netstat -anp).png" width="50%;" />

  + **netstat 옵션**

    + -a 옵션 : 현재 다른 PC와 연결되어 있거나 대기(Listening) 중인 모든 port 확인
    + -r 옵션 : 라우팅 테이블 확인 / 커넥션되어 있는 port 확인
    + -n 옵션 : 현재 다른 PC와 연결되어 있는 port 확인
    + -e 옵션 : 랜카드에서 송수신한 패킷의 용량/종류 확인
    + -s 옵션 : IP, ICMP, UDP 프로토콜별 상태 확인
    + -t 옵션 : tcp protocol
    + -u 옵션 : udp protocol
    + -p 옵션 : 프로토콜 사용 Process ID 확인
    + ic 옵션 : 1초 단위로 보여줌

<br/>

## CNI(Container Network Interface)

### Pod Networking

+ 쿠버네티스는 kubenet이라는 기본적인 network plugin을 제공하지만, cross node network or network policy 설정과 같은 고급 기능은 구현되어 있지 않음 → network plugin을 따로 사용해야 함

+ 쿠버네티스는 모든 pod들이 unique한 ip 주소를 갖고, 해당 ip를 통해 같은 노드의 모든 pod들이 서로 통신할 수 있게 함

+ Networking Solution(ex. Flannel, NSX, ...)은 network를 구성할 필요 없이 자동으로 IP 주소를 할당하고 노드 및 다른 노드의 pod 간의 연결을 설정함

+ Pod Networking(클러스터 내부/외부의 pod 간 통신 방법)

  <img src="https://user-images.githubusercontent.com/33214969/161440589-5ca5850b-bd3a-4002-962f-868f89cb9454.png" alt="Network - Pod networking.png" width="40%;" />

  1. 노드에 컨테이너에 생성되면 network namespace를 생성함 → 쿠버네티스는 통신을 위해 namespace를 network에 붙임(이때, network = bridge network) ⇒ 각 노드에 bridge network를 생성함 각 노드에 Bridge network를 생성 + 기동(bring up)

     ```bash
     $ ip link add v-net-0 type bridge
     $ ip link set dev v-net-0 up     # 10.244.1.0/24로 할당
     $ addr add 10.244.1.1/24 dev v-net -0
     ```

  2. 각 bridge network, bridge interface에 ip 주소를 할당함 + 컨테이너를 network에 붙임. → pod는 각각 ip 주소를 할당받아 서로 통신할 수 있게 됨

  + veth(Virtual Ethernet Devices) : 한 쌍의 인터페이스가 Peer로 서로 연결된 가상 인터페이스

    + ip 주소 할당 → `ip link add`를 통해 생성할 수 있음 → pod들이 unique ip를 갖음 + 서로 통신할 수 있음 script 작성([net-script.sh](http://net-script.sh))
    + 컨테이너에 network 연결 → pipe or virtual network table을 사용함

    ```coffeescript
    # net-script.sh
    # veth pair 생성
    $ ip link add [veth명] type veth peer name [peer-name]
    
    # veth pair 연결
    $ ip link set up [veth명]
    
    # ip 주소 할당
    $ ip -n [namespace명] addr add ...
    $ ip -n [namespace명] route add ...
    
    # 인터페이스 bring up
    $ ip -n [namespace명] link add ...
    ```
  
  3. routing table을 이용해 모든 host에 대해 route를 구성함 → 다른 노드의 pod들과 통신 가능하도록
  
     ```coffeescript
     # in node1(129.168.1.11)
     $ ip route add 10.244.2.2 via 192.168.1.12
     # ping 10.244.2.2 -> node1 to node2 OK
     
     $ ip route add 10.244.4.3 via 192.168.1.13
     # ping 10.244.3.2 -> node1 to node3 OK
     ```
  
  4. CNI 사용
  
     <img src="https://user-images.githubusercontent.com/33214969/161440590-f79c8598-bc07-418e-b36c-7a034fa73bbe.png" alt="Network - Pod networking2.png" width="40%;" />
  
     + 10.244.0.0/16의 단일 대규모 network를 형성함 → CNI를 사용
     + CNI가 쿠버네티스에게 컨테이너를 작성하는 즉시 script를 호출하도록 요청함 + CNI는 컨테이너 추가/삭제를 담당하는 섹션을 추가/삭제해야 함
     + kubelet의 역할
       1. CNI 설정(Configuration)을 확인
       2. script 이름을 식별
       3. `/etc/cni/bin` 디렉토리를 검색
       4. add 명령과 컨테이너 이름 or namespace id를 사용하여 script를 실행

### CNI(Container Network Interface)

```tex
▫️ 컨테이너 간의 네트워킹을 제어할 수 있는 plugin
```

+ 특징
  + 컨테이너 작성을 담당하는 쿠버네티스 내의 컴포넌트에 의해 CNI plugin이 호출됨
  + 컨테이너가 생성된 후 해당 구성 요소가 적절한 network plugin을 호출함

### CNI in Kubernetes

+ kubelet을 통해 CNI 정보 확인 : `ps -aux | grep kubelet`

  ```bash
  $ ps -aux | grep kubelet
  # --network-plugin=cni
  # --cni-bin-dir=/opt/cni/bin
  # --cni-confi-dir=/etc/cni/net.d  -> net.d 안에 bridge.conf가 설정되어 있음
  ```

  + `--network-plugin=cni` : kubelet 실행 시, 받은 CNI 설정 정보
  + `--cni-bin-dir` : CNI 플러그인 파일 디렉토리
  + `--cni-confi-dir` : CNI 설정 파일 디렉토리

+ kubelet은 위의 정보들을 통해 CNI script의 add 명령어를 수행하여 network를 구성함

### CNI weave

```tex
▫️ CNI plugin의 한 종류
```

+ 배경

  + routing table은 수많은 entry를 지원할 수 없음 → 클러스터 내 노드가 너무 많고, 각 노드에 pod가 많은 환경에서는 다른 솔루션이 필요함

+ 특징

  + weave CNI plugin이 클러스터에 배포 > plugin이 각 노드에 agent or service를 배포함
  + weave는 노드, 네트워크, pod에 대한 정보를 교환하기 위해 서로 통신함
  + 각 agent or peer는 전체 설정의 topology를 저장함 → 다른 노드에 있는 pod와 노드의 ip를 알 수 있음
  + weave는 노드들에 각각 bridge를 생성 + weave라고 이름을 지정함 + 각 network에 ip 주소를 할당함
  + pod는 여러 bridge network에 연결할 수 있음 ex) pod가 docker에 만든 docker bridge 뿐만 아니라 seave bridge에도 붙을 수 있음
  + 패킷이 대상에 도달하기 위해 사용하는 경로는 컨테이너에 구성된 경로에 따라 다름

+ weave 동작방식

  <img src="https://user-images.githubusercontent.com/33214969/161440575-9c77ca79-d8cc-42f8-8f41-c0839d5dff97.png" alt="Network - CNI.png" width="50%;" />

  1. weave는 pod들이 agent에 도달할 수 있도록 구성된 올바른 경로를 확보하도록 함 + agent는 다른 pod들을 처리함
  2. 패킷이 하나의 pod에서 다른 노드의 다른 pod로 보내지면, weave는 패킷을 가로채고 별도의 network에 있음을 확인함
  3. 그 다음 이 패킷을 새로운 소스/대상이 있는 새 패킷으로 캡슐화 + network를 통해 전송함
  4. 다른 weave agent가 패킷을 검색/분해하여 패킷을 올바른 pod로 라우팅함

+ 쿠버네티스 클러스터에 weave 배포 방법

  + weave 및 weave peer는 클러스터 내의 각 노드에 service or daemon으로 배포될 수 있음
  + 만약, 쿠버네티스가 이미 설정되어 있을 경우, kubectl apply 명령어를 사용하여 클러스터에 pod로 배포될 수 있음 → `kubectl apply -f "<https://cloud.weave.works/k8s/net?k8s-version=$>(kubectl version | base64 | tr -d '\\n')&env.IPALLOC_RANGE=10.50.0.0/16"`

+ 관련 명령어

  + 네트워크 plugin 확인 : `ps -aux | grep kubelet` → `--network-plugin` 항목 확인
  + CNI의 binary로 구성된 경로 확인 : `ps -aux | grep kubelet` → `--cni-bin-dir` 항목 확인
  + host에서 사용가능한 CNI plugin 목록 확인 : `ls /opt/cni/bin` → 해당 폴더에서 사용 가능한 폴더(plugin) 확인
  + CNI plugin 확인 : `ls /etc/cni/net.d`
  + 컨테이너 or namespace 생성 후 kubelet에서 실행할 binary 실행 파일 확인 : `cat /etc/cni/net.d/[CNI plugin 파일명].confist` → `plugins.type` 항목 확인

  **weave 배포**

  + host 시스템의 ip 주소 확인 : `ip a`
  + weave pod 확인 : `kubectl get po -n kube-system | grep weave`
  + weave log 확인 : `kubectl logs -n kube-system [weave pod명] -c weave`
  + &env를 사용하여 기본 IP 주소(10.50.0.0)를 변경 : `kubectl apply -f "<https://cloud.weave.works/k8s/net?k8s-version=$>(kubectl version | base64 | tr -d '\\n')&env.IPALLOC_RANGE=10.50.0.0/16"`

<br/>

## Networking Weave

+ pod ip를 중복되지 않도록 할당하는 방법
  + ip 리스트를 파일로 저장하는 방법 → 가장 간단한 방법 → 이 파일을 각 host에 두고 ip를 관리할 수 있음
  + weave가 ip 주소를 관리하는 방법 → weave는 전체 network에 10.32.0.0/12에 해당하는 범위의 ip를 할당함 → 이는 10.32.0.1 ~ 10.47.255.254 범위에 해당함 → peer가 ip 주소를 동등하게 분할하여 각 노드에 할당함 → 해당 노드에서 생성된 pod는 해당 범위를 갖게 됨

### IPAM

```tex
▫️ ip 할당 관리
```

+ ex) DHCP, host-local, ...

  <img src="https://user-images.githubusercontent.com/33214969/161440588-d085f084-6d7c-4f10-8f83-934870b4171f.png" alt="Network - IPAM.png" width="50%;" />

+ conf 파일

  ```bash
  $ cat /etc/cni/net.d/net-script.conf
  
  # net-script.conf
  # {
  # 	"cniVersion": "0.2.0",
  # 	"name": "mynet",
  # 	"type": “net-script",
  # 	"bridge": "cni0",
  # 	"isGateway": true,
  # 	"ipMasq": true,
  # 	"ipam": {    # IPAM 부분
  # 		"type": "host-local",
  # 		"subnet": "10.244.0.0/16",
  # 		"routes": [
  # 					{ "dst": "0.0.0.0/0" }
  # 				]
  # 	}
  # }
  ```

+ pod의 ip 주소 확인 : (동작중인 컨테이너의 shell에 접근) `kubectl exec -ti [pod명] -- sh` / (ip 확인) : `ip route`

<br/>

## Service Networking

+ Service → 다른 pod에 호스팅된 service에 액세스하기 위해 사용함

  + ClusterIP → service가 생성되면 pod 종류와 관계없이 클러스터의 모든 pod에서 액세스가 가능함 → 클러스터 전체에서 service가 host될 때, pod가 노드에 host되지만, 특정 노드에 바인딩되지는 않음 → 클러스터 내에서만 service에 접근이 가능함
  + NodePort → NodePort 또한 할당된 ip 주소를 이용하여 ClusterIP와 동일하게 동작함 → 클러스터 내의 모든 노드의 port에 애플리케이션을 표시(expose)하므로 → 외부 사용자 or 애플리케이션이 service에 접근할 수 있음

+ Service가 어떻게 ip 주소를 갖고, 어떻게 클러스터의 모든 노드에서 사용할 수 있는지?

  <img src="https://user-images.githubusercontent.com/33214969/161440597-cf5fbc5c-beb2-40c7-8a05-3c83d1d04738.png" alt="Network - Service Networking(iptables)2.png" width="50%;" />

  + service의 개념

    + 모든 노드는 kubelet을 이용해 pod를 생성함 → 각 노드의 각 Kubelet은 apiserver를 통해 클러스터의 변경을 감시 + 새로운 pod가 생성될 때마다 노드에 pod를 생성함
    + 그 다음, CNI plugin을 호출하여 해당 pod의 networking을 구성함 + 각 노드는 kube-proxy을 실행함
    + kube-proxy는 kube-apiserver를 통해 클러스터의 변경을 감시함 + 새로운 service가 생성될 때마다 kube-proxy가 동작함
    + pod와 달리 service는 각 노드에서 생성되거나 or 각 노드에 할당되지 않음 → service는 클러스터 전체의 개념임(=클러스터 내의 모든 노드에 걸쳐 존재함)
    + service의 ip를 실제로 수신하고 있는 서버 or service는 없음. service를 위한 process, namespace, interface도 없음 → service는 가상의 객체임

  + service의 ip 취득 방법

    + service 객체와 코디네이터(coordinator)를 작성하면 미리 정의된 범위의 ip 주소가 할당됨
    + 각 노드에서 실행중인 kube-proxy는 service ip를 가져와서 클러스터 내의 각 노드에서 전송 규칙(forwarding rules)을 생성함 → 해당 ip에 도달하는 트래픽은 모두 pod의 ip로 전송되어야 함
    + pod의 ip가 1개 있으면 pod가 service의 ip에 도달하려고 할 때마다 클러스터의 모든 노드에서 접근할 수 있는 pod의 ip로 전달됨
    + service가 생성/삭제될 때마다 kube-module 컴포넌트는 이러한 규칙을 생성/삭제함
    + service가 생성될 때 할당되는 ClusterIP → kube-apiserver 옵션을 통해 정의된 범위 내에서 할당됨

  + kube-proxy

    <img src="https://user-images.githubusercontent.com/33214969/161440595-21d8ee93-95af-4ecf-8f8a-561c34a28ee2.png" alt="Network - Service Networking(iptables).png" width="50%;" />

    + forwarding rule 생성 방법 3가지
      1. userspace
      2. iptables(default)
      3. ipvs

  + iptables

    + kube-proxy의 iptable

      | IP:Port    | 10.99.13.178:80 |
      | ---------- | --------------- |
      | Forward to | 10.244.1.2      |

    + pod의 iptable

      | Pod DB                | 10.244.1.2          |
      | --------------------- | ------------------- |
      | Service DB(ClusterIP) | 10.103.132.104:3306 |

    + ip 범위 확인

      <img src="https://user-images.githubusercontent.com/33214969/161440594-fe61f027-8123-4925-9854-2dc846af5d78.png" alt="Network - Service Networking(ip range).png" width="50%;" />

+ 관련 명령어

  + kube-proxy의 forwarding rule 생성 : `kube-proxy --proxy-mode [upserspace | iptagles | ipvs] ...`
  + kube-proxy의 forwarding rule 확인 : `iptables -L -t nat | grep [service명]`
  + ip 범위 확인(`ipcalc` 사용)
    + ipcalc 설치 : `apt update` + `apt install ipcalc`
    + ip 범위 확인 : `ipcalc -b [ip]`
  + 클러스터의 node에 대해 구성된 ip 범위 확인 : `kubectl get nodes -o wide` → ip 확인 + `ip a` → 해당 ip 대역대를 확인
  + 클러스터의 pod에 대해 구성된 ip 범위 확인 : `kubectl logs [weave pod명] -c weave -n kube-system` → ipalloc-range 확인
  + 클러스터 내의 service에 대해 구성된 ip 확인 : `cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range` / `kube-api-server --service-cluster-ip-range ipNet` + `ps aux | grep kube-api-server`의 `range` 항목
  + kube-proxy의 proxy 타입 확인 : `kubectl logs [kube-proxy의 pod명] -n kube-system`

<br/><br/>