# 쿠버네티스 아키텍처

<img src="https://user-images.githubusercontent.com/33214969/161391237-7cc943b9-3084-4331-8c70-678960a3119f.png" alt="쿠버네티스 아키텍처.png" width="70%;" />

<br/>

# 1-1. 쿠버네티스 아키텍처(Kubernetes Architecture)

## 노드(Node)

```tex
▫️ 컨테이너가 배포될 물리 서버 or 가상 머신
```

+ 특징

  + 클러스터의 관리 대상으로 등록된 도커 호스트로, 도커 컨테이너가 배치되는 대상
  + 워커 노드(Worker Node)라고도 부름
  + 쿠버네티스 클러스터 전체를 관리하는 서버인 마스터가 적어도 하나 이상 있어야 함
  + 실제 운영(product) 환경에서는 최소 3개 이상의 마스터 노드를 갖는 것이 좋음

+ 아키텍처

  <img src="https://user-images.githubusercontent.com/33214969/161391343-c0eea294-2f0c-4282-b5b2-6b0cb3351c00.png" alt="노드 - 쿠버네티스 노드 아키텍처.png" width="70%;" />

<br/>

## 마스터 노드(Master Node)

```tex
▫️ 쿠버네티스 클러스터 전체를 컨트롤하는 시스템
```

<img src="https://user-images.githubusercontent.com/33214969/161391455-7d909b8a-c14a-4e3d-ab55-13f199ad368d.png" alt="노드 - 마스터노드.png" width="70%;" />

+ 특징

  + 마스터는 API 서버를 통해 쿠버네티스를 관리하고, 모든 컴포넌트들은 API 서버를 통해 통신함
  + 관리자만 접속 가능 + 보안 설정 필요
  + 마스터 서버가 죽으면 클러스터를 관리할 수 없음 → 3대를 구성하여 안전성을 높임

+ 구성 요소

  1. Kube-apiserver

     ```tex
     ▫️ 쿠버네티스의 모든 기능들은 REST API로 제공하는데, 그에 대한 명령을 처리하는 부분
     ```

     + 특징

       + 쿠버네티스 내부의 모든 컴포넌트들이 서로 호출하기 위해 사용하는 컴포넌트
       + kubectl 요청 뿐만 아니라 내부 모듈의 요청 처리, 권한 체크, 요청 거부, etcd의 데이터 검색&업데이트를 수행
       + etcd와 직접 상호 작용하는 유일한 구성요소임
       + 노드에서 실행중인 컨테이너의 로그를 확인하는 디버거 역할도 가능

     + 동작방식

       1. kubectl 명령 실행 > kubectl utility가 kube-apiserver에 도달함
       2. kube-apiserver는 먼저 요청을 인증 & 유효성을 검사함
       3. etcd 클러스터에서 데이터를 검색 & 맞는 데이터를 응답함
       4. 스케줄러는 kube-apiserver를 계속 모니터링함
       5. 스케줄러가 할당된 노드가 없는 새 포드가 있는 것을 확인하면 > 새로운 포드를 배치할 올바른 노드를 식별 후 > 이를 다시 kube-apiserver에 전달함
       6. kube-apiserver는 etcd 클러스터에 정보를 업데이트함
       7. kube-apiserver는 해당 정보를 적절한 워커 노드의 kebelet에 전달함
       8. kubelet은 노드에 pod를 생성함 + 컨테이너 런타임 엔진(ex. docker)에 애플리케이션 이미지를 배포하도록 지시함
       9. 완료되면 kebelet은 상태를 다시 kube-apiserver에 전달함 + kube-apiserver는 etcd 클러스터의 데이터를 다시 업데이트함

     + apiserver 주소 확인 및 호출

       + apiserver 주소 확인

       + $HOME/.kube/config 위치에 있는 kubeconfig 파일 확인

       + apiserver 주소 = `cluster[0].cluster.server`

         ```yaml
         apiVersion: v1
         clusters:
         - cluster:
             certificate-authority-data: xxxx
             server: <https://10.0.0.1:6443>
           name: cluster.local
         contexts:
         - context:
         ...
         ```

       + apiserver 주소 호출

         ```bash
         $ curl <https://10.0.1:6443>
         ```

         + `# curl: (60) SSL certificate problem: unable to get local issuer certificate`* 에러 → apiserver가 사용하는 서버 인증서가 공식 CA(Certificate Authority)에서 발급한 정식 인증서가 아니기 때문에 발생. 사용자가 서버 인증서를 검증할 수 있는 자체 CA 인증서인kubeconfig의 certificate-authority-data property를 사용하여 apiserver를 호출해야 함

         ```bash
         # kubeconfig 파일로부터 CA 인증서를 추출하는 방법
         $ kubectl config view --minify --raw --output 'jsonpath={..cluster.certificate-authority-data}' | base64 -d > k8s-ca.cert
         
         # --cacert 옵션으로 사용자가 명시적으로 CA 인증서를 제공함
         $ curl --cacert k8s-ca.cert <https://10.0.0.1:6443>
         
         # -k 혹은 --insecure (서버 인증서 확인 skip)
         curl -k <https://10.0.0.1:6443>
         # 위와 동일한 결과
         ```

         + `403 status code` 에러 → apiserver로부터 적절한 접근권한이 없는 경우 발생함

     + 명령어

       + Install

         ```bash
         # Linux
         $ wget <https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-apiserver>
         
         # MacOS
         $ brew install kubectl
         # or
         $ brew install kubernetes-cli
         ```

       + 설정 확인

         ```bash
         # 버전 확인
         $ kubectl version --client
         
         # kube-apiserver 옵션 확인
         $ kubectl get pods -n kube-system
         $ cat /etc/kubernetes/manifests/kube-apiserver.yaml
         $ cat /etc/systemd/system/kube-apiserver.service
         $ ps -aux | grep kube-apiserver
         ```

       + API 서버 호출

         ```bash
         # kubectl이 바라보는 apiserver의 주소를 출력
         $ kubectl cluster-info
         # Kubernetes control plane is running at <https://10.0.0.1:6443>
         ```

  2. ETCD

     ```tex
     ▫️ 쿠버네티스 클러스터의 모든 설정, 상태 데이터를 key-value 형태로 저장하는 저장소(DB)
     ```

     + 특징

       + object를 key-value 형태로 저장함
       + 노드, pods, configs, secrets, accounts, roles, bindings, ... 등과 같은 클러스터 관련 정보 + 노드 추가, pod 배포 등과 같은 모든 변경사항들을 저장함
       + 나머지는 모두 stateless하게 동작하기 때문에 ETCD만 잘 백업하면 언제든지 클러스터 복구가 가능함
       + 여러 개로 분산하여 → 복제 가능 + 안전성 높음 + 속도 빠름
       + 단순히 값을 저장 + 읽기 뿐만 아니라 watch 기능으로 상태 변경 체크가 가능함
       + 오직 API 서버와 통신함 + 다른 모듈들은 API 서버를 거쳐서 ETCD 데이터에 접근함
       + HA(High Availavility, 고가용성)임<br/>(* HA : 시스템이 상당히 오랜기간 동안 지속적으로 정상 운영이 가능한 컴포넌트 or 시스템)

     + 명령어

       + Install

         ```bash
         # Linux
         $ curl -L <https://github.com/etcd> io/etcd/releases/download/v3.3.11/etcd v3.3.11 linux amd64.tar.gz o etcd v3.3.11 linux amd64.tar.gz
         $ tar xzvf etcd v3.3.11 linux amd64.tar.gz
         $ ./etcd    # etcd 실행
         
         # MacOS
         $ ruby -e "$(curl -fsSL <https://raw.githubusercontent.com/Homebrew/install/master/install>)" 2> /dev/null
         $ brew install etcd
         
         # (참조) <https://github.com/etcd-io/etcd/releases>
         ```

       + 설정 확인

         ```bash
         # etcd 버전 확인
         $ ./etcdctl version  # etcdctl -> etcd cli
         # etcdctl version: 3.4.9
         # API version: 3.4
         
         # etcd 설정 확인
         $ etcd.service
         ```

       + 운영

         ```bash
         # 쿠버네티스에서 etcd 정보 조회하기
         $ kubectl get pod -n (namespace명)
         # NAME          READY   STATUS       RESTARTS   AGE    ...
         # etcd-node001  1/1     Running      0          229d
         # etcd-node002  1/1     Running      0          229d
         # etcd-node003  1/1     Running      1          229d
         
         # pod의 이벤트 확인
         $ kubectl describe pod (pod명)
         # Name:         nginx
         # Namespace:    default
         # Priority:     0
         # ...
         # Events:
         #   Type    Reason     Age    From                 Message
         #   ----    ------     ----   ----                 -------
         #   Normal  Scheduled  2m37s  default-scheduler    Successfully assigned default/nginx to worker-001
         #   Normal  Pulling    2m36s  kubelet, worker-001  Pulling image "nginx:1.19.2"
         #   Normal  Created    2m28s  kubelet, worker-001  Created container nginx
         
         # 쿠버네티스에 있는 모든 키 조회(etcdctl 사용)
         $ kubectl exec etcd-master -n (namespace명) etcdctl get / --prefix -keys-only
         # /registry/pods/istio-system/istio-citadel-58c6fb56fb-fs2gp
         # /registry/services/endpoints/istio-system/istio-citadel
         # ...
         ```

       + Key-Value

         ```bash
         # Key-Value 생성
         $ ./etcdctl set (key값) (value값)
         $ ./etcdctl put (key값) (value값)
         # OK
         
         # Key-Value 조회
         $ ./etcdctl get key
         # key
         # value
         
         # Key-Value 삭제
         $ ./etcdctl del key
         # 1
         ```

  3. 스케줄러(Scheduler)

     ```tex
     ▫️ pod, 서비스 등 각 리소스들을 적절한 노드에 할당하는 역할
     ```

     + 특징
       + 스케줄러는 노드에 할당되지 않은 새로 생성된 pod가 있는지 모니터링함
       + 노드 할당이 필요한 pod를 발견하면, pod를 실행할 수 있는 최적의 노드를 찾는 작업을 진행함 > 노드에 배치된 pod는 각 노드의 kubelet에 의해 컨테이너로 생성됨
       + pod들이 리소스에 대해 각기 다른 요구사항을 가지고 있음 → 상세 스케줄링 요구사항에 따른 노드 필터링이 필요함
       + 이때 상세 스케줄링 요구사항에 맞는 노드들을 적합한 노드들(feasible nodes)라고 함
       + 적합한 노드가 없는 pod는 적합한 위치를 찾을 때까지 unscheduled 상태로 남아있음
     + 스케줄링 과정
       1. Filtering : pod에 적합한 노드를 찾는 과정
       2. Scoring : Filtering에 의해 찾은 노드 중 가장 적합한 노드를 선별하는 과정
     + Binding 과정
       1. 스케줄러가 적합한 노드를 찾음
       2. Scoring을 위해 사전에 정의된 함수들을 이용하여 노드의 점수를 계산
       3. 가장 높은 점수를 받은 노드를 선정하여 apiserver에게 알림
     + 스케줄링 시 고려 사항
       + individual and collective resource requirements
       + H/W
       + S/W
       + policy constraints
       + affinity and anti-affinity specifications
       + data locality
       + inter-workload interference
       + etc

  4. 컨트롤러 매니저(Kube Controll Manager)

     ```tex
     ▫️ 컨트롤러 프로세스를 실행하는 컨트롤 플레인 컴포넌트
     ```

     + 특징

       + 쿠버네티스의 ReplicaSet, Deployment 등 Controller를 관리하고 적절한 노드에 할당하는 역할
       + 논리적으로 각 컨트롤러는 분리된 프로세스이지만, 복잡성을 낮추기 위해 모두 단일 바이너리로 컴파일 & 단일 프로세스 내에서 실행됨

     + Controller

       + Controller<br/> - 시스템 내의 다양한 구성 요소의 상태를 지속적으로 모니터링 + 전체 시스템을 원하는 작동 상태로 유지하는 역할을 함
         + Node Controller<br/> - 노드 상태를 모니터링 + 애플리케이션 실행을 유지하기 위해 필요한 조치를 취함<br/> - 5초마다 노드의 상태를 모니터링함<br/> - 노트에 연결할 수 없다고 표시되면 40초 동안 기다림 + 이후에도 동일한 경우, 노드 위에 할당된 pod를 제거 & 정상적인 노드로 대체함
         + Replication Controller<br/> - Replica sets의 상태를 모니터링 + Replica sets에서 원하는 수의 pod를 사용할 수 있도록 유지<br/> - 만약 pod가 죽으면 새로운 pod를 생성함
         + 그 외<br/> - Deployment, Service-Account, Namespace, Endpoint, Job, Stateful-Set, Replca set, ...

     + 명령어

       + Install

         ```bash
         # Linux
         $ wget <https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-controller-manager>
         ```

       + 설정 확인

         ```bash
         # kube-controller-manager cluster 확인
         $ kubectl get pods -n kube-system
         
         # kube-controller-manager 옵션 확인
         $ cat /etc/kubernetes/manifests/kube-controller-manager.yaml
         $ cat /etc/systemd/system/kube-controller-manager.service
         $ ps -aux | grep kube-controller-manager
         ```

  5. 클라우드 컨트롤러 매니저(Cloud Controller Manager)

     ```tex
     ▫️ AWS, GCE, Azure 등 public 클라우드와 연동/관리하는 역할
     ```

     + 특징
       + 노드를 추가/삭제하고 로드밸런서를 연결 or 볼륨을 붙임
       + 각 클라우드 업체의 인터페이스에 맞춰 구현할 수 있음

  6. DNS

     ```tex
     ▫️ 내부 위치 정보를 저장하는 DNS 서버
     ```

     + 특징
       + 리소스 endpoint를 DNS로 맵핑 + 관리
       + pod or 서비스들은 ip를 배정 받지만, 동적으로 생성되는 리소스이기 때문에 ip가 변경되면 DNS에 위치 정보를 다시 저장함
       + 새로운 리소스가 생기면 그 리소스에 대한 ip와 DNS 이름을 등록함 + DNS 이름을 기반으로 리소스에 접근함

<br/>

## 워커 노드(Worker Node)

```tex
▫️ 마스터에 의해 명령을 받고 실제 워크 로드를 생성해서 서비스하는 컴포넌트
```

<img src="https://user-images.githubusercontent.com/33214969/161391458-3ab60783-985b-4dda-bf22-2857d3712016.png" alt="노드 - 워커노드.png" width="50%;" />

+ 특징

  + 실제 컨테이너들이 생성되는 가상 머신 or 물리적인 서버
  + 워커 노드를 여러 개 생성해서 관리 가능함
  + 각각의 서버에 레이블을 붙여서 사용 목적을 정의할 수 있음
  + API 서버의 요청을 Kubelet을 통해 수행함

+ 구성 요소

  1. Kubelet(큐블릿)

     ```tex
     ▫️ 마스터의 API 서버와 통신하는 역할
     ```

     + 특징
       + 마스터의 API 서버와 통신하며 노드가 수행할 명령을 받아서 워커 노드를 수행시킴
       + 워커 노드의 상태를 마스터로 전달함
       + 노드에 할당된 pod들의 생명주기를 관리함
       + pod를 생성함 + pod 안의 컨테이너에 이상이 없는지 확인하며 마스터와 통신함
       + API 서버의 요청을 받아서 컨테이너의 로그를 전달하거나 or 특정 명령을 대신 수행하기도 함
       + PodSpecs라는 설정을 받아서 그 조건에 맞게 컨테이너를 실행 + 컨테이너가 정상적으로 실행되고 있는지 상태 체크를 진행함

  2. Kube-proxy

     ```tex
     ▫️ 노드에 할당된 pod로 연결되는 네트워크를 관리하는 역할
     ```

     + 특징
       + 노드로 오는 트래픽을 적절한 컨테이너로 프록시하고, 노드 <-> 마스터 간의 네트워크 통신을 관리함
       + 클러스터 내부에 별도의 가상 네트워크가 동작할 수 있게 해주는 프로세스
       + 노드로 들어오는 트래픽을 적절한 컨테이너로 라우팅, 로드밸런싱, 프록시하면서 통신을 관리함
       + kube-proxy 자체가 프록시 서버로 동작함 → 실제 요청을 받고 pod에게 전달하는 역할을 함
     + 기타 특징
       + TCP, UDP, SCTP 스트림을 포워딩 + 여러 개의 pod를 라운드로빈 형태로 묶어서 서비스를 제공함
       + 시간이 지나면서 iptables를 설정하는 방식으로 변경됨 → 그런데 iptables에 등록된 규칙도 느려지면서 최근 IPVS를 지원함

  3. 컨테이너 런타임(Container Runtime)

     ```tex
     ▫️ pod를 통해 배포된 컨테이너를 실제로 실행시키는 역할
     ```

     + 특징
       + 대표적인 예시 : 도커(Docker)
       + 그 외 rkt, runc와 같은 런타임도 지원함
       + CRI(Container Runtime Interface)를 구현한 다양한 런타임을 지원함

  4. 애드온(Addons)

     ```tex
     ▫️ 클러스터 내부에서 필요한 기능을 실행하기 위한 pod
     ```

     + 특징

       + 이 pod들은 deployment controller, replication controller에 의해 관리됨
       + 에드온이 사용하는 네임스페이스 = `kube-system`

     + 종류

       1. 네트워킹 애드온
          + 쿠버네티스를 직접 보유 중인 서버들에 설치하여 사용하는 경우 → 애드온 별도 설치 필요
          + AWS, GCP와 같은 클라우드 서비스에서 제공하는 쿠버네티스를 사용하는 경우 → 애드온 설치 필요 X
          + CNI(Container Network Interface)를 구현하고 있으면 다른 애드온들도 사용 가능함

       2. DNS 애드온
          + 실제로 클러스터 내에서 작동하는 DNS 서버
          + 쿠버네티스 서버들에게 DNS 레코드를 제공하는 역할
          + 쿠버네티스 내부에서 실행된 컨테이너들은 자동으로 DNS 서버에 등록됨

       3. 대시보드 애드온
          + 웹 UI가 필요한 경우 사용할 수 있는 대시보드

  5. cAdvisor

     ```tex
     ▫️ 노드의 모니터링 에이전트(agent)
     ```

     + 특징
       + 노드에서 가동되는 컨테이너들의 상태 정보를 수집하여 마스터의 API 서버로 전달함

<br/><br/>