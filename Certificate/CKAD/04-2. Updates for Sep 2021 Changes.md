# 04-2. Updates for Sep 2021 Changes

## Custom Resource Definition

### Custom Resource

```tex
▫️ 직접 정의하여 사용할 수 있는 리소스
```

+ Custom Resource 추가 방법 2가지
  1. CRD 사용 (프로그래밍 X)
  2. API aggregation 사용 (프로그래밍 O → 데이터 저장 방법, API 버전 변화 등 API 동작을 보다 강력하게 제어 가능)
+ Custom Resource를 사용하기 위한 방법
  1. 현재 상태를 Custom Resource에 대해 바람직한 상태(desire state)로 변화시킬 수 있는 컨트롤러 구현 + 실행
  2. Custom Resource의 상세 정보를 정의하는 CRD(Custom Resource Definition)를 생성
  3. CRD에서 정의된 데이터에 맞춰 Custom Resource를 생성
  4. 위의 컨트롤러는 Custom Resource의 생성을 모니터링 + Custom Resource가 원하는 바람직한 상태(desire state)가 되도록 적절한 작업을 수행

### Custom Resource Definition

```tex
▫️ Custom Resource를 정의하기 위한 개체
```

+ 특징

  + Custom Resource를 etcd에 등록하기 위해 사용됨
  + Custom Resource를 처리하기 위해 직접 API server를 작성할 필요 X → API server aggregation보다 유연성이 떨어짐

+ yaml 파일

  ```yaml
  apiVersion: apiextensions.k8s.io/v1
  kind: CustomResourceDefinition
  metadata:
    name: crontabs.stable.example.com   # CRD 이름
  spec:
    group: stable.example.com  # 커스텀 리소스의 API 그룹(REST API: /apis/<group>/<version>)
    scope: Namespaced          # 커스텀 리소스가 네임스페이스 포함 여부
    names:
      plural: crontabs    # 커스텀 리소스의 이름 (복수형)
      singular: crontab   # 커스텀 리소스의 이름 (단수형)
      kind: CronTab       # YAML 파일 등에서 사용될 커스텀 리소스의 Kind
      shortNames:
  		- ct                # 커스텀 리소스 이름의 줄임말
    versions:
    - name: v1                 # 커스텀 리소스의 API 버전
      served: true
      storage: true
      schema:
        openAPIV3Schema:       # 커스텀 리소스의 데이터를 정의
          type: object
          required: ["spec"]   # 필수 필드값
          properties:          # 커스텀 리소스에 저장될 데이터 형식을 정의
            spec:
              required: ["myvalue"]
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: "string
                replicas:
                  type: integer
                  minimum: 1   # 데이터의 range 정의 -> 만약 벗어나는 경우 커스텀 리소스 생성 불가
  ```

  ```yaml
  # Custom Resource
  apiVersion: "stable.example.com/v1"
  kind: CronTab
  metadata:
    name: my-new-cron-object
  spec:
    cronSpec: "* * * * */5"
    image: my-awesome-cron-image
  ```

<br/>

## Operator Framework

```tex
▫️ Custom Resource Definition(CRD)와 Custom Resource를 관리하는 Custom Controller를 단일 객체로 배포할 수 있도록 패키징한 것
```

+ 특징
  + Operator를 배포함으로써 내부적으로 CRD 및 Resource를 생성 + Controller를 Deployment로 배포할 수 있음
+ ETCD Operator : Kubernetes 내의 ETCD 클러스터를 배포/관리하는 Operator
  + ETCD 클러스터와 CRD, Custom Controller가 포함되어 있음
  + Kubernetes 클러스터에 ETCD를 배포한 후, ETCD 클러스터 리소스를 모니터링 함
  + backup 및 restore operator를 이용해 ETCD 클러스터를 백업을 복구하는 등 장애 발생 시, 애플리케이션 전반에서 발생할 수 있는 문제 해결 작업을 수행함
  + 특정 애플리케이션을 설치/유지보수하는 등의 작업을 수행함

<br/>

## Deployment strategies(Blue/Green, Canary)

### Blue/Green

```tex
▫️ 구버전(Blue), 신버전(Green) 2세트의 서버를 마련하고 한꺼번에 교체하는 배포 방법
```

+ 특징

  + 기존에 실행된 Pod 개수만큼 신규 Pod를 모두 실행함 + 신규 Pod가 정상적으로 실행되면 한꺼번에 트래픽을 옮기는 방식
  + 신버전과 구버전이 같이 존재하는 시간 없이 순간적인 교체가 가능함
  + Rolling Update보다 필요한 리소스 양이 많음

+ 구현 방법

  1. 구버전 애플리케이션 배포 = blue → (lable) `version=v1`

     + yaml

       ```yaml
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: myapp-blue
         labels:
           app: myapp
           type: front-end
       spec:
         templete:
           metadata:
             name: myapp-pod
             labels:
               version: v1 # 서비스 연결용 Label
           spec: containers
           - name: app-container
             image: myapp-image:1.0
         replicas: 5
         selector:
           matchLabels:
             type: front-end
       ```

  2. blue로 트래픽을 라우팅하는 서비스를 생성 → (selector) `version=v1`

     + yaml

       ```yaml
       apiVersion: v1
       kind: Service
       metadata:
         name: my-service
       spec:
         selector:
           version: v1 # 포드 연결용 Selector
       ```

  3. 신버전 애플리케이션 배포 = green → (lable) `version=v2`

     + yaml

       ```yaml
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: myapp-green
         labels:
           app: myapp
           type: front-end
       spec:
         templete:
           metadata:
             name: myapp-pod
             labels:
               version: v2  # 신버전
           spec: containers
           - name: app-container
             image: myapp-image:2.0
         replicas: 5
         selector:
           matchLabels:
             type: front-end
       ```

  4. 신버전 테스트 완료 후 서비스의 Selector를 v2로 변경 → (selector) `version=v2`

     + yaml

       ```yaml
       apiVersion: v1
       kind: Service
       metadata:
         name: my-service
       spec:
         selector:
           version: v2  # 신버전으로 트래픽을 변경
       ```

<img src="https://user-images.githubusercontent.com/33214969/212492718-9a018a91-a6b9-449d-8ecb-354c8d9e0367.png" alt="디플로이먼트-Blue:Green.png" width="50%;" />

### Canary

```tex
▫️ 신버전의 일부를 배포하고, 대부분의 트래픽은 구버전을 바라보게하는 배포 방법
```

+ 특징

  + 기존 버전을 유지한 채로 일부 버전만 신규 파드로 교체 (한꺼번에 전체 교체 X)
  + 구버전과 신버전이 같이 존재함
  + 버그 확인, 사용자 반응 확인할 때 유용함

+ 구현 방법

  1. 구버전의 애플리케이션을 Deployment로 배포 → deployment-primary

     + yaml

       ```yaml
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: myapp-primary
         labels:
           app: myapp
           type: front-end
       spec:
         templete:
           metadata:
             name: myapp-pod
             labels:
               version: v1
               app: front-end # 서비스 연결용 Label
           spec: containers
           - name: app-container
             image: myapp-image:1.0
         replicas: 5
         selector:
           matchLabels:
             type: front-end
       ```

  2. Deployment에 트래픽을 라우팅하는 서비스를 생성 + 서비스와 Pod 연결을 위해 Label, Selector `app=front-end`을 설정함

     + yaml

       ```yaml
       apiVersion: v1
       kind: Service
       metadata:
         name: my-service
       spec:
         selector:
           app: front-end # 포드 연결용 Selector
       ```

  3. 새 버전의 애플리케이션을 배포 → deployment-canary

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: myapp-canary
       labels:
         app: myapp
         type: front-end  # Service의 Label과 동일
     spec:
       templete:
         metadata:
           name: myapp-pod
           labels:
             version: v2
             app: front-end
         spec: containers
         - name: app-container
           image: myapp-image:2.0
       replicas: 1   # 구버전의 replicas보다 작게 설정
       selector:
         matchLabels:
           type: front-end
     ```

  4. 두 버전 모두 트래픽을 라우팅할 수 있도록 Service의 Selector 설정이 Deployment의 Label로 설정되어 있어야 함 + 새 버전에 트래픽이 적어야 하므로 Replicas를 구 버전보다 적게 설정

  5. 새 버전 테스트 완료 후 구 버전을 신 버전으로 업그레이드(Rolling Update)

<img src="https://user-images.githubusercontent.com/33214969/212492742-73dcba21-4820-417a-af1c-b3544d860553.png" alt="디플로이먼트-Canary.png" width="50%;" />

<br/>

## Helm

```tex
▫️ Kubernetes용 패키지 매니지먼트 도구
```

+ 기존의 Kubernetes는 클러스터에 선언된 객체/리소스를 개별로 관리함 → 이를 패키징하여 관리하는 것이 helm임

+ 특징

  + 패키지 = Kubernetes 리소스를 하나로 묶은 helm chart
  + helm chart = YAML 파일의 묶음(패키지)
  + helm chart를 public or private registry에 push해놓고, helm 명령어를 활용하여 helm chart를 설치하여 Kubernetes 리소스를 배포할 수 있음

+ 패키지 설치

  ```bash
  $ helm install [RELEASE명] [PACKAGE]
  ```

  + helm은 설치 마법사와 같이 애플리케이션의 모든 객체 + yaml 파일 등을 클러스터 내에 추가함
  + release : Kubernetes 클러스터에서 동작하는 패키지의 인스턴스에 해당함
  + 하나의 패키지는 동일 클러스터 내에 중복으로 설치 가능 + 설치할 때마다 새로운 release가 생성됨
  + 설치 시 원하는 구성을 위해 설정을 추가할 수 있음 → values.yaml 파일로 관리됨

+ Release 버전 관리

  ```bash
  # -f : 저장한 파일을 반영하여 업그레이드
  $ helm upgrade -f [CONFIG_FILE] [RELEASE명] [PACKAGE]
  $ helm rollback [RELEASE명] [VERSION]
  ```

  + 설치한 애플리케이션을 upgrade/rollback 명령어를 이용하여 버전 관리
  + install, upgrade, rollback이 실행될 때마다 release 버전이 자동으로 1씩 증가함

+ Release 삭제

  ```bash
  $ helm uninstall [RELEASE명]
  ```

### Install Package

+ 전제 조건

  1. 동작중인 Kubernetes Cluster
  2. 적절한 자격 증명이 구성된 Kubectl

+ Snap in Linux

  ```bash
  $ snap install helm --classic
  ```

+ apt in Debian/Ubuntu

  ```bash
  # key or Source Resource를 추가
  $ curl <https://baltocdn.com/helm/signing.asc> | sudo apt-key add -
  $ apt-get install apt-transport-https --yes
  $ echo "deb <https://baltocdn.com/helm/stable/debian/> all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
  $ apt-get update
  $ apt-get install helm
  ```

+ pkg

  ```bash
  $ pkg install helm
  ```

### Helm Concepts

+ values.yaml

  + helm은 values.yaml 파일을 통해 하나의 패키지로 여러 환경을 구성할 수 있도록 함

    + 값을 변수로 취급할 수 있도록 파일을 템플릿으로 변환함
    + 변수는 중괄호로 감싸서 나타냄 + 값을 식별할 변수명을 적음

  + 템플릿 + values.yaml = Kubernetes가 클러스터에 애플리케이션을 배포할 때 사용하는 정의파일

  + yaml

    ```yaml
    # values.yaml
    image: redis
    ```

    ```yaml
    # pod.yaml
    apiVersion: v1
    kind: Pod
    metadata:
    	name: mypod
    spec:
    	containers:
    	- name: redis
    		image: {{ .Values.image }}
    ```

    + 최종 Pod의 yaml

      ```yaml
      # pod.yaml
      apiVersion: v1
      kind: Pod
      metadata:
      	name: mypod
      spec:
      	containers:
      	- name: redis
      		image: redis  # values.yaml의 변수 적용
      ```

+ helm chart = template + values.yaml + Chart.yaml

  + Chart.yaml → 차트 이름, 버전, 설명과 같은 차트 관련 정보들이 저장되어 있음
  + 사용자는 helm chart를 직접 생성 or 차트 저장소(https://artifacthub.io/)에 공개된 차트를 사용

+ 관련 명령어

  + hub에서 chart 검색 : `helm search hub [이름]`
  + 다른 저장소 추가 : `helm repo add [Repo명] [URL]`
  + 클라이언트 저장소에서 chart 검색 : `helm search repo [PACAKGE명]`
  + 클라이언트 저장소 확인 : `helm repo list`
  + 차트 다운로드 : `helm pull --untar [CHART명]`

  + `--untart` 옵션 : 차트를 로컬로 다운 + 압축 해제. 압축 해제된 경로는 chart명과 같으며, 디렉터리 내 values.yaml을 편집하여 설치에 사용함

+ Helm Release : 차트가 설치된 버전

  + 동일한 차트로 동일한 애플리케이션을 중복하여 설치할 수 있음

  + release 이름은 차트 설치 시, 지정할 수 있음

  + 설치 명령어를 실행할 때마다 release가 생성됨

    <img src="https://user-images.githubusercontent.com/33214969/212492802-c34753f4-8ddc-4d37-841c-6a4f2f1e78ce.png" alt="helm chart.png" width="30%;" />

  + 관련 명령어

    + 차트 설치 : `helm install [RELEASE명] [CHART명]`, (지정한 디렉터리 내 파일로 설치) `helm install [RELEASE명] [DIR]`
    + release 설치 목록 확인 : `helm list`
    + release 삭제 : `helm uninstall [RELEASE명]`

+ 관련 명령어

  + 설치된 OS 정보 확인 : `cat /etc/*release*`

  + helm 설치 : https://helm.sh/docs/intro/install/#from-apt-debianubuntu

    ```bash
    $ curl <https://baltocdn.com/helm/signing.asc> | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
    $ sudo apt-get install apt-transport-https --yes
    $ echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] <https://baltocdn.com/helm/stable/debian/> all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
    $ sudo apt-get update
    $ sudo apt-get install helm
    ```

  + 도움말 : `helm help`

  + 버전 확인 : `helm version`

  + hub에서 차트 검색 : `helm search hub [CHART명]`

  + node에 차트 repo 추가 : `helm repo add [REPO명] [CHART REPO명]` → ex) helm repo add bitnami https://charts.bitnami.com/bitnami

  + 설치된 repo에서 차트 검색 : `helm search repo [CHART명]`

  + repo 리스트 조회 : `helm repo list`

  + 특정 repo에서 차트 설치 :`helm install [RELEASE명] [CHART명]`

  + package 리스트 조회 : `helm list`

  + release 삭제 : `helm uninstall [RELEASE명]`

  + 지정한 디렉터리의 파일로 차트 다운로드 : `helm pull --untar [CHART명]`

  + 다운로드 받은 chart를 설치 : `helm install [RELEASE명] [다운받은 chart위치]`